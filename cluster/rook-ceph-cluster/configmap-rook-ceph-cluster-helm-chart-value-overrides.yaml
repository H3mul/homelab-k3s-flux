apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-ceph-cluster-helm-chart-value-overrides
  namespace: rook-ceph
data:
  values.yaml: |-  
    # Paste from:
    # https://github.com/rook/rook/blob/10abbd0d8a0ff2e5330ceaca733a31eec4391a5a/deploy/charts/rook-ceph-cluster/values.yaml

    # Default values for a single rook-ceph cluster
    # This is a YAML-formatted file.
    # Declare variables to be passed into your templates.

    # -- Namespace of the main rook operator
    operatorNamespace: rook-ceph

    # -- The metadata.name of the CephCluster CR
    # @default -- The same as the namespace
    clusterName:

    # -- Optional override of the target kubernetes version
    kubeVersion:

    # -- Cluster ceph.conf override
    configOverride: |
      [global]
      # mon_allow_pool_delete = true
      osd_pool_default_size = 1
      osd_pool_default_min_size = 1

    # Installs a debugging toolbox deployment
    toolbox:
      # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
      enabled: true
      # -- Toolbox image, defaults to the image used by the Ceph cluster
      image: quay.io/ceph/ceph:v17.2.5
      # -- Toolbox tolerations
      tolerations: []
      # -- Toolbox affinity
      affinity: {}
      # -- Toolbox resources
      # resources:
      #   limits:
      #     cpu: "500m"
      #     memory: "1Gi"
      #   requests:
      #     cpu: "100m"
      #     memory: "128Mi"
      # -- Set the priority class for the toolbox if desired
      # priorityClassName:

    monitoring:
      # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
      # Monitoring requires Prometheus to be pre-installed
      enabled: true
      # -- Whether to create the Prometheus rules for Ceph alerts
      createPrometheusRules: false
      # -- The namespace in which to create the prometheus rules, if different from the rook cluster namespace.
      # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
      # deployed) to set rulesNamespace for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
      rulesNamespaceOverride:
      # Monitoring settings for external clusters:
      # externalMgrEndpoints: <list of endpoints>
      # externalMgrPrometheusPort: <port>
      # allow adding custom labels and annotations to the prometheus rule
      prometheusRule:
        # -- Labels applied to PrometheusRule
        labels: {}
        # -- Annotations applied to PrometheusRule
        annotations: {}

    # -- Create & use PSP resources. Set this to the same value as the rook-ceph chart.
    pspEnable: false

    # imagePullSecrets option allow to pull docker images from private docker registry. Option will be passed to all service accounts.
    # imagePullSecrets:
    # - name: my-registry-secret

    # All values below are taken from the CephCluster CRD
    # -- Cluster configuration.
    # @default -- See [below](#ceph-cluster-spec)
    cephClusterSpec:
      # This cluster spec example is for a converged cluster where all the Ceph daemons are running locally,
      # as in the host-based example (cluster.yaml). For a different configuration such as a
      # PVC-based cluster (cluster-on-pvc.yaml), external cluster (cluster-external.yaml),
      # or stretch cluster (cluster-stretched.yaml), replace this entire `cephClusterSpec`
      # with the specs from those examples.

      # For more details, check https://rook.io/docs/rook/v1.10/CRDs/Cluster/ceph-cluster-crd/
      cephVersion:
        # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
        # v16 is Pacific, v17 is Quincy.
        # RECOMMENDATION: In production, use a specific version tag instead of the general v16 flag, which pulls the latest release and could result in different
        # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
        # If you want to be more precise, you can always use a timestamp tag such quay.io/ceph/ceph:v15.2.11-20200419
        # This tag might not contain a new Ceph version, just security fixes from the underlying operating system, which will reduce vulnerabilities
        image: quay.io/ceph/ceph:v17.2.5
        # Whether to allow unsupported versions of Ceph. Currently `pacific` and `quincy` are supported.
        # Future versions such as `reef` (v18) would require this to be set to `true`.
        # Do not set to true in production.
        allowUnsupported: false

      # The path on the host where configuration files will be persisted. Must be specified.
      # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
      # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
      dataDirHostPath: /var/lib/rook

      # Whether or not upgrade should continue even if a check fails
      # This means Ceph's status could be degraded and we don't recommend upgrading but you might decide otherwise
      # Use at your OWN risk
      # To understand Rook's upgrade process of Ceph, read https://rook.io/docs/rook/v1.10/Upgrade/ceph-upgrade/
      skipUpgradeChecks: false

      # Whether or not continue if PGs are not clean during an upgrade
      continueUpgradeAfterChecksEvenIfNotHealthy: false

      # WaitTimeoutForHealthyOSDInMinutes defines the time (in minutes) the operator would wait before an OSD can be stopped for upgrade or restart.
      # If the timeout exceeds and OSD is not ok to stop, then the operator would skip upgrade for the current OSD and proceed with the next one
      # if `continueUpgradeAfterChecksEvenIfNotHealthy` is `false`. If `continueUpgradeAfterChecksEvenIfNotHealthy` is `true`, then operator would
      # continue with the upgrade of an OSD even if its not ok to stop after the timeout. This timeout won't be applied if `skipUpgradeChecks` is `true`.
      # The default wait timeout is 10 minutes.
      waitTimeoutForHealthyOSDInMinutes: 10

      mon:
        # Set the number of mons to be started. Generally recommended to be 3.
        # For highest availability, an odd number of mons should be specified.
        count: 1
        # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
        # Mons should only be allowed on the same node for test environments where data loss is acceptable.
        allowMultiplePerNode: true

      mgr:
        # When higher availability of the mgr is needed, increase the count to 2.
        # In that case, one mgr will be active and one in standby. When Ceph updates which
        # mgr is active, Rook will update the mgr services to match the active mgr.
        count: 1
        allowMultiplePerNode: false
        modules:
          # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
          # are already enabled by other settings in the cluster CR.
          - name: pg_autoscaler
            enabled: true

      # enable the ceph dashboard for viewing cluster status
      dashboard:
        enabled: true
        # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
        # urlPrefix: /ceph-dashboard
        # serve the dashboard at the given port.
        port: 8443
        # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
        # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
        ssl: true

      # Network configuration, see: https://github.com/rook/rook/blob/master/Documentation/CRDs/ceph-cluster-crd.md#network-configuration-settings
      network:
        connections:
          # Whether to encrypt the data in transit across the wire to prevent eavesdropping the data on the network.
          # The default is false. When encryption is enabled, all communication between clients and Ceph daemons, or between Ceph daemons will be encrypted.
          # When encryption is not enabled, clients still establish a strong initial authentication and data integrity is still validated with a crc check.
          # IMPORTANT: Encryption requires the 5.11 kernel for the latest nbd and cephfs drivers. Alternatively for testing only,
          # you can set the "mounter: rbd-nbd" in the rbd storage class, or "mounter: fuse" in the cephfs storage class.
          # The nbd and fuse drivers are *not* recommended in production since restarting the csi driver pod will disconnect the volumes.
          encryption:
            enabled: false
          # Whether to compress the data in transit across the wire. The default is false.
          # Requires Ceph Quincy (v17) or newer. Also see the kernel requirements above for encryption.
          compression:
            enabled: false
          # Whether to require communication over msgr2. If true, the msgr v1 port (6789) will be disabled
          # and clients will be required to connect to the Ceph cluster with the v2 port (3300).
          # Requires a kernel that supports msgr v2 (kernel 5.11 or CentOS 8.4 or newer).
          requireMsgr2: false
      #   # enable host networking
      #   provider: host
      #   # EXPERIMENTAL: enable the Multus network provider
      #   provider: multus
      #   selectors:
      #     # The selector keys are required to be `public` and `cluster`.
      #     # Based on the configuration, the operator will do the following:
      #     #   1. if only the `public` selector key is specified both public_network and cluster_network Ceph settings will listen on that interface
      #     #   2. if both `public` and `cluster` selector keys are specified the first one will point to 'public_network' flag and the second one to 'cluster_network'
      #     #
      #     # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
      #     #
      #     # public: public-conf --> NetworkAttachmentDefinition object name in Multus
      #     # cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
      #   # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
      #   ipFamily: "IPv6"
      #   # Ceph daemons to listen on both IPv4 and Ipv6 networks
      #   dualStack: false

      # enable the crash collector for ceph daemon crash collection
      crashCollector:
        disable: false
        # Uncomment daysToRetain to prune ceph crash entries older than the
        # specified number of days.
        # daysToRetain: 30

      # enable log collector, daemons will log on files and rotate
      logCollector:
        enabled: true
        periodicity: daily # one of: hourly, daily, weekly, monthly
        maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.

      # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
      cleanupPolicy:
        # Since cluster cleanup is destructive to data, confirmation is required.
        # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
        # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
        # Rook will immediately stop configuring the cluster and only wait for the delete command.
        # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
        confirmation: ""
        # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
        sanitizeDisks:
          # method indicates if the entire disk should be sanitized or simply ceph's metadata
          # in both case, re-install is possible
          # possible choices are 'complete' or 'quick' (default)
          method: quick
          # dataSource indicate where to get random bytes from to write on the disk
          # possible choices are 'zero' (default) or 'random'
          # using random sources will consume entropy from the system and will take much more time then the zero source
          dataSource: zero
          # iteration overwrite N times instead of the default (1)
          # takes an integer value
          iteration: 1
        # allowUninstallWithVolumes defines how the uninstall should be performed
        # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
        allowUninstallWithVolumes: false

      # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
      # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
      # tolerate taints with a key of 'storage-node'.
      # placement:
      #   all:
      #     nodeAffinity:
      #       requiredDuringSchedulingIgnoredDuringExecution:
      #         nodeSelectorTerms:
      #           - matchExpressions:
      #             - key: role
      #               operator: In
      #               values:
      #               - storage-node
      #     podAffinity:
      #     podAntiAffinity:
      #     topologySpreadConstraints:
      #     tolerations:
      #     - key: storage-node
      #       operator: Exists
      #   # The above placement information can also be specified for mon, osd, and mgr components
      #   mon:
      #   # Monitor deployments may contain an anti-affinity rule for avoiding monitor
      #   # collocation on the same node. This is a required rule when host network is used
      #   # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
      #   # preferred rule with weight: 50.
      #   osd:
      #   mgr:
      #   cleanup:

      # annotations:
      #   all:
      #   mon:
      #   osd:
      #   cleanup:
      #   prepareosd:
      #   # If no mgr annotations are set, prometheus scrape annotations will be set by default.
      #   mgr:

      # labels:
      #   all:
      #   mon:
      #   osd:
      #   cleanup:
      #   mgr:
      #   prepareosd:
      #   # monitoring is a list of key-value pairs. It is injected into all the monitoring resources created by operator.
      #   # These labels can be passed as LabelSelector to Prometheus
      #   monitoring:

      resources: {}
        # mgr:
        #   limits:
        #     cpu: "1000m"
        #     memory: "1gi"
        #   requests:
        #     cpu: "500m"
        #     memory: "512mi"
        # mon:
        #   limits:
        #     cpu: "2000m"
        #     memory: "2gi"
        #   requests:
        #     cpu: "1000m"
        #     memory: "1gi"
        # osd:
        #   limits:
        #     cpu: "2000m"
        #     memory: "4gi"
        #   requests:
        #     cpu: "1000m"
        #     memory: "4gi"
        # prepareosd:
        #   # limits: it is not recommended to set limits on the osd prepare job
        #   #         since it's a one-time burst for memory that must be allowed to
        #   #         complete without an oom kill.  note however that if a k8s
        #   #         limitrange guardrail is defined external to rook, the lack of
        #   #         a limit here may result in a sync failure, in which case a
        #   #         limit should be added.  1200mi may suffice for up to 15ti
        #   #         osds ; for larger devices 2gi may be required.
        #   #         cf. https://github.com/rook/rook/pull/11103
        #   requests:
        #     cpu: "500m"
        #     memory: "50mi"
        # mgr-sidecar:
        #   # limits:
        #   #   cpu: "500m"
        #   #   memory: "100mi"
        #   requests:
        #     cpu: "100m"
        #     memory: "40mi"
        # crashcollector:
        #   # limits:
        #   #   cpu: "500m"
        #   #   memory: "60mi"
        #   requests:
        #     cpu: "100m"
        #     memory: "60mi"
        # logcollector:
        #   # limits:
        #   #   cpu: "500m"
        #   #   memory: "1gi"
        #   requests:
        #     cpu: "100m"
        #     memory: "100mi"
        # cleanup:
        #   # limits:
        #   #   cpu: "500m"
        #   #   memory: "1gi"
        #   requests:
        #     cpu: "500m"
        #     memory: "100mi"

      # the option to automatically remove osds that are out and are safe to destroy.
      removeosdsifoutandsafetoremove: false

      # priority classes to apply to ceph resources
      priorityclassnames:
        mon: system-node-critical
        osd: system-node-critical
        mgr: system-cluster-critical

      storage: # cluster level storage configuration and selection
        useallnodes: false
        usealldevices: false
        # devicefilter: sdb
        # config:
        #   crushroot: "custom-root" # specify a non-default root label for the crush map
        #   metadatadevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
        #   databasesizemb: "1024" # uncomment if the disks are smaller than 100 gb
        #   journalsizemb: "1024"  # uncomment if the disks are 20 gb or smaller
        #   osdsperdevice: "1" # this value can be overridden at the node or device level
        #   encrypteddevice: "true" # the default value for this option is "false"
        # # individual nodes and their config can be specified as well, but 'useallnodes' above must be set to false. then, only the named
        # # nodes below will be used as storage resources. each node's 'name' field should match their 'kubernetes.io/hostname' label.

        nodes:
          - name: ofnir
            devices:
              - name: "nvme0n1"
                config:
                  deviceclass: ssd

          - name: mimir
            devices:
              - name: "nvme0n1"
                config:
                  deviceclass: ssd

              # - name: "/dev/sdb"
              #   config:
              #     deviceclass: hdd

        # nodes:
        #   - name: maliketh
        #     devices: # specific devices to use for storage can be specified for each node
        #       - name: "sdb"
        #       # - name: "nvme01" # multiple osds can be created on high performance devices
        #       #   config:
        #       #     osdsperdevice: "5"
        #       - name: "/dev/disk/by-id/ata-st4000dm004-xxxx" # devices can be specified using full udev paths
        #     config: # configuration can be specified at the node level which overrides the cluster level config
        #   - name: "172.17.4.301"
        #     devicefilter: "^sd."

      # the section for configuring management of daemon disruptions during upgrade or fencing.
      disruptionmanagement:
        # if true, the operator will create and manage poddisruptionbudgets for osd, mon, rgw, and mds daemons. osd pdbs are managed dynamically
        # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). the operator will
        # block eviction of osds by default and unblock them safely when drains are detected.
        managepodbudgets: true
        # a duration in minutes that determines how long an entire failuredomain like `region/zone/host` will be held in `noout` (in addition to the
        # default down/out interval) when it is draining. this is only relevant when  `managepodbudgets` is `true`. the default value is `30` minutes.
        osdmaintenancetimeout: 30
        # a duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and osds came back up.
        # operator will continue with the next drain if the timeout exceeds. it only works if `managepodbudgets` is `true`.
        # no values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
        pghealthchecktimeout: 0

      # configure the healthcheck and liveness probes for ceph pods.
      # valid values for daemons are 'mon', 'osd', 'status'
      healthcheck:
        daemonhealth:
          mon:
            disabled: false
            interval: 45s
          osd:
            disabled: false
            interval: 60s
          status:
            disabled: false
            interval: 60s
        # change pod liveness probe, it works for all mon, mgr, and osd pods.
        livenessprobe:
          mon:
            disabled: false
          mgr:
            disabled: false
          osd:
            disabled: false

    ingress:
      # -- enable an ingress for the ceph-dashboard
      dashboard:
        annotations:
          ingress.kubernetes.io/protocol: https
        #   external-dns.alpha.kubernetes.io/hostname: dashboard.example.com
        #   kubernetes.io/ingress.class: nginx
        # if the dashboard has ssl: true the following will make sure the nginx ingress controller can expose the dashboard correctly
        #   nginx.ingress.kubernetes.io/backend-protocol: "https"
        #   nginx.ingress.kubernetes.io/server-snippet: |
        #     proxy_ssl_verify off;
        host:
          name: &host ceph.${CLUSTER_DOMAIN}
        tls:
          - secretname: letsencrypt-wildcard-cert-prod
            hosts:
              - *host
        ## note: only one of ingress class annotation or the `ingressclassname:` can be used at a time
        ## to set the ingress class
        ingressclassname: traefik

    # -- a list of cephblockpool configurations to deploy
    # @default -- see [below](#ceph-block-pools)
    cephblockpools:
      - name: ceph-blockpool
        # see https://github.com/rook/rook/blob/master/documentation/crds/block-storage/ceph-block-pool-crd.md#spec for available configuration
        spec:
          failuredomain: host
          replicated:
            size: 1
          # enables collecting rbd per-image io statistics by enabling dynamic osd performance counters. defaults to false.
          # for reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
          # enablerbdstats: true
        storageclass:
          enabled: true
          name: ceph-block
          isdefault: true
          reclaimpolicy: delete
          allowvolumeexpansion: true
          volumebindingmode: "immediate"
          mountoptions: []
          # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
          allowedtopologies: []
    #        - matchlabelexpressions:
    #            - key: rook-ceph-role
    #              values:
    #                - storage-node
          # see https://github.com/rook/rook/blob/master/documentation/ceph-block.md#provision-storage for available configuration
          parameters:
            # (optional) mapoptions is a comma-separated list of map options.
            # for krbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
            # for nbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
            # mapoptions: lock_on_read,queue_depth=1024

            # (optional) unmapoptions is a comma-separated list of unmap options.
            # for krbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
            # for nbd options refer
            # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
            # unmapoptions: force

            # rbd image format. defaults to "2".
            imageformat: "2"

            # rbd image features, equivalent to or'd bitfield value: 63
            # available for imageformat: "2". older releases of csi rbd
            # support only the `layering` feature. the linux kernel (krbd) supports the
            # full feature complement as of 5.4
            imagefeatures: layering

            # these secrets contain ceph admin credentials.
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .release.namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .release.namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .release.namespace }}"
            # specify the filesystem type of the volume. if not specified, csi-provisioner
            # will set default as `ext4`. note that `xfs` is not recommended due to potential deadlock
            # in hyperconverged settings where the volume is mounted on the same node as the osds.
            csi.storage.k8s.io/fstype: ext4

    # -- a list of cephfilesystem configurations to deploy
    # @default -- see [below](#ceph-file-systems)
    cephfilesystems:
      - name: ceph-filesystem
        # see https://github.com/rook/rook/blob/master/documentation/crds/shared-filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
        spec:
          metadatapool:
            replicated:
              size: 1
              name: cephfs.storage.metadata
          datapools:
            # - failuredomain: host
            - replicated:
                size: 1
              # optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/documentation/crds/shared-filesystem/ceph-filesystem-crd.md#pools
              name: cephfs.storage.data
          metadataserver:
            activecount: 1
            activestandby: true
            # resources:
            #   limits:
            #     cpu: "2000m"
            #     memory: "4gi"
            #   requests:
            #     cpu: "1000m"
            #     memory: "4gi"
            priorityclassname: system-cluster-critical
        storageclass:
          enabled: true
          isdefault: false
          name: ceph-filesystem
          # (optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
          pool: cephfs.storage.data
          reclaimpolicy: delete
          allowvolumeexpansion: true
          volumebindingmode: "immediate"
          mountoptions: []
          # see https://github.com/rook/rook/blob/master/documentation/ceph-filesystem.md#provision-storage for available configuration
          parameters:
            # the secrets contain ceph admin credentials.
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .release.namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .release.namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .release.namespace }}"
            # specify the filesystem type of the volume. if not specified, csi-provisioner
            # will set default as `ext4`. note that `xfs` is not recommended due to potential deadlock
            # in hyperconverged settings where the volume is mounted on the same node as the osds.
            csi.storage.k8s.io/fstype: ext4

    # -- settings for the filesystem snapshot class
    # @default -- see [cephfs snapshots](../storage-configuration/ceph-csi/ceph-csi-snapshot.md#cephfs-snapshots)
    cephfilesystemvolumesnapshotclass:
      enabled: false
      name: ceph-filesystem
      isdefault: true
      deletionpolicy: delete
      annotations: {}
      labels: {}
      # see https://rook.io/docs/rook/v1.10/storage-configuration/ceph-csi/ceph-csi-snapshot/#cephfs-snapshots for available configuration
      parameters: {}

    # -- settings for the block pool snapshot class
    # @default -- see [rbd snapshots](../storage-configuration/ceph-csi/ceph-csi-snapshot.md#rbd-snapshots)
    cephblockpoolsvolumesnapshotclass:
      enabled: false
      name: ceph-block
      isdefault: false
      deletionpolicy: delete
      annotations: {}
      labels: {}
      # see https://rook.io/docs/rook/v1.10/storage-configuration/ceph-csi/ceph-csi-snapshot/#rbd-snapshots for available configuration
      parameters: {}

    # -- a list of cephobjectstore configurations to deploy
    # @default -- see [below](#ceph-object-stores)
    cephobjectstores:
      - name: ceph-objectstore
        # see https://github.com/rook/rook/blob/master/documentation/crds/object-storage/ceph-object-store-crd.md#object-store-settings for available configuration
        spec:
          metadatapool:
            # failuredomain: host
            replicated:
              size: 1
          datapool:
            replicated:
              size: 1
            # failuredomain: host
            # erasurecoded:
            #   datachunks: 2
            #   codingchunks: 1
          preservepoolsondelete: true
          gateway:
            port: 80
            # resources:
            #   limits:
            #     cpu: "2000m"
            #     memory: "2gi"
            #   requests:
            #     cpu: "1000m"
            #     memory: "1gi"
            # secureport: 443
            # sslcertificateref:
            instances: 1
          healthcheck:
            bucket:
              interval: 60s
        storageclass:
          enabled: true
          name: ceph-bucket
          reclaimpolicy: delete
          volumebindingmode: "immediate"
          # see https://github.com/rook/rook/blob/master/documentation/ceph-object-bucket-claim.md#storageclass for available configuration
          parameters:
            # note: objectstorenamespace and objectstorename are configured by the chart
            region: us-west-1
